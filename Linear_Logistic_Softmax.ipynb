{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Logistic_Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBgXErBard6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "98e63635-64de-4dd7-beea-228bb6dddc5c"
      },
      "source": [
        "# Google Colab required\n",
        "import math\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"5\" # export OMP_NUM_THREADS=5\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"5\" # export OPENBLAS_NUM_THREADS=5\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"5\" # export MKL_NUM_THREADS=5\n",
        "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"5\" # export VECLIB_MAXIMUM_THREADS=5\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"5\" # export NUMEXPR_NUM_THREADS=5\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "from scipy.integrate import odeint, solve_ivp\n",
        "from scipy.linalg import expm, qr\n",
        "import scipy.io as sio\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "device = torch.device('cpu')\n",
        "import copy\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "import random\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve('https://github.com/MerkulovDaniil/split-sgd/blob/master/Code/lls_data/fanlinear.mat', 'fanlinear.mat')\n",
        "urllib.request.urlretrieve('https://github.com/MerkulovDaniil/split-sgd/blob/master/Code/lls_data/shepplogan.mat', 'shepplogan.mat')\n",
        "# Reproducibility\n",
        "random.seed(999)\n",
        "np.random.seed(999)\n",
        "torch.manual_seed(999)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "print('Libraries imported successfuly!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries imported successfuly!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3fqF8resswF",
        "colab_type": "text"
      },
      "source": [
        "# Linear "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_RPtOvJsMqs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f901b601-7c96-4e71-ce63-f849f5bd0bbe"
      },
      "source": [
        "TARGET_RES = 16e-5 # desired loss rate\n",
        "N_EXPERIMENTS = 11\n",
        "LEARNING_RATES = np.array(np.logspace(-0.2, 0.8, 15))\n",
        "iter_limit = 10000 # limit to iterations due to not converging\n",
        "GD_N_iter = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SGD_N_iter = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SPL_N_iter = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "GD_time = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SGD_time = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SPL_time = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "p = 100 \n",
        "n = 1000 # random data array [n x p]\n",
        "s = 50 # split count\n",
        "b = 20 # batch size\n",
        "epsilon = 1e-2 # noise rate, min noise converge early\n",
        "problems = [\n",
        "            #'tomography',\n",
        "            'random'\n",
        "            ] # dataset types\n",
        "print('Problem parameters defined successfuly!')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem parameters defined successfuly!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuYfwTxbtIQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bbc22fab-8fa5-4c3d-8ecf-e0de88123202"
      },
      "source": [
        "# Load Tom LLS data\n",
        "def loadTomData():\n",
        "    X = sio.loadmat(os.path.join(os.path.abspath(os.curdir) ,'fanlinear.mat'))[\"A\"].toarray()\n",
        "    theta_true = sio.loadmat(os.path.join(os.path.abspath(os.curdir) ,'shepplogan.mat'))[\"x\"]\n",
        "    y = np.squeeze(X @ theta_true)\n",
        "    return X, theta_true, y\n",
        "\n",
        "# Creating random X [n x p]\n",
        "# Formula definition\n",
        "# Create y with noise\n",
        "def generateRandomProblem(p, n, lstsq=False, eps = 0):\n",
        "    X = np.random.randn(n, p)\n",
        "    theta_clean = np.ones(p)\n",
        "    y = X @ theta_clean + eps * np.random.randn(n) \n",
        "    init_bound = 1.0/math.sqrt(p)\n",
        "    theta_0 = np.array(init_bound*torch.FloatTensor(p).uniform_(-1, 1))\n",
        "    if lstsq == False:\n",
        "        return X, theta_0, y\n",
        "    else:\n",
        "        theta_lstsq = np.linalg.lstsq(X,y)[0]\n",
        "        return X, theta_0, y, theta_lstsq\n",
        "\n",
        "# C.1.2 EXACT SOLUTION OF THE LOCAL PROBLEM\n",
        "def exactSolution(Q, R, theta_0, y_batch, h, n):\n",
        "    try:\n",
        "        R_it = np.linalg.inv(R.T)\n",
        "    except:\n",
        "        R_it = np.linalg.pinv(R.T)\n",
        "    return Q @ ( expm(-1/n* R @ R.T*h) @ (Q.T @ theta_0 - R_it @ y_batch )) + Q @ (R_it @ y_batch) + theta_0 - Q @ (Q.T @ theta_0)\n",
        "\n",
        "# C.1.3 formula 21 - exact formula for Kaczmarz method\n",
        "def KaczmarzMethod(x, theta_0, y, h, n):\n",
        "    x = x.T\n",
        "    norm = x.T @ x\n",
        "    return theta_0 + (1 - np.exp(-norm*h/n))*(y - x.T @ theta_0)/norm*x\n",
        "\n",
        "# Loss function of Linear Least Sq\n",
        "def loss(X, theta, y):\n",
        "    if len(X.shape) == 2:\n",
        "        n, _ = X.shape\n",
        "        return 1/n*np.linalg.norm(X @ theta - y)**2\n",
        "    elif len(X.shape) == 3:\n",
        "        s, b, _ = Xs.shape\n",
        "        n = b * s\n",
        "        loss = 0\n",
        "        for i in range(s):\n",
        "            loss += 1/n*np.linalg.norm(X[i] @ theta - y[i])**2\n",
        "        return loss\n",
        "    else:\n",
        "        print('HITTITITIITITITIIT')\n",
        "        raise ValueError('Data Shape is not Suitable!!!')\n",
        "\n",
        "# || calculated - exact || / || exact || \n",
        "# True values to compare with Loss\n",
        "# Calculated for all batches at once\n",
        "def exactValues(X, theta, y):\n",
        "    if len(X.shape) == 2:\n",
        "        return np.linalg.norm(X @ theta - y)/np.linalg.norm(y)\n",
        "    elif len(X.shape) == 3:\n",
        "        s, b, p = Xs.shape\n",
        "        n = b * s\n",
        "        loss = 0\n",
        "        X_full = np.zeros((n, p))\n",
        "        y_full = np.zeros(n)\n",
        "        for i in range(s):\n",
        "            y_full[b*i:b*(i+1)] = y[i]\n",
        "            X_full[b*i:b*(i+1),:] = X[i]\n",
        "        return np.linalg.norm(X_full @ theta - y_full)/np.linalg.norm(y_full)\n",
        "    else:\n",
        "        print('HITTITITIITITITIIT')\n",
        "        raise ValueError('Data Shape is not Suitable!!!')\n",
        "\n",
        "# C.1.1 formula 17 gradient update\n",
        "def calculateGradient(X, theta, y):\n",
        "    n, p = X.shape\n",
        "    return (1/n * (X.T @ (X @ theta - y)))\n",
        "\n",
        "# SGD iteration step\n",
        "def SGDIteration(X_batch, theta_0, y_batch, lr):\n",
        "    theta = theta_0 - lr * calculateGradient(X_batch, theta_0, y_batch)\n",
        "    return theta\n",
        "\n",
        "# train SGD \n",
        "def trainSGD(thetaTemp, Xs, ys, lr, treshold = 1e-4, maxIter = 1000):\n",
        "    s, b, _ = Xs.shape\n",
        "    n = b * s\n",
        "    iterCount = 0\n",
        "    stopFlag = False\n",
        "    losses = []\n",
        "    if lr > 9:\n",
        "        maxIter = 10000\n",
        "    while not stopFlag:          \n",
        "        i_batch = iterCount % s\n",
        "        losses.append(loss(Xs, thetaTemp, ys))\n",
        "        thetaTemp = SGDIteration(Xs[i_batch], thetaTemp, ys[i_batch], lr)\n",
        "        iterCount += 1\n",
        "        if iterCount % 50 == 0:\n",
        "            print(f'SGD exactValues {exactValues(Xs, thetaTemp, ys):.5f}, losses {losses[-1]:.3f}/{treshold:.5f}, iteration {iterCount}. Lr {lr}')\n",
        "        if losses[-1] <= treshold:\n",
        "            stopFlag = True\n",
        "            break\n",
        "        if iterCount >= maxIter:\n",
        "            stopFlag = True\n",
        "            iterCount = None\n",
        "    if iterCount == None:\n",
        "      print(f'\\n SGD complete with fail. iteration {iterCount}, lr {lr}')\n",
        "    else:\n",
        "      print(f'\\n SGD complete with success. iteration {iterCount}, lr {lr}')\n",
        "    return iterCount\n",
        "\n",
        "# train splitting \n",
        "def trainSplitting(thetaTemp, Qs, Rs, Xs, ys, stepsize, treshold = 1e-4, maxIter = 1000):\n",
        "    s, b, _ = Xs.shape\n",
        "    n = b * s\n",
        "    iterCount = 0\n",
        "    stopFlag = False\n",
        "    losses = []\n",
        "    while not stopFlag:          \n",
        "        i_batch = iterCount % s\n",
        "        losses.append(loss(Xs, thetaTemp, ys))\n",
        "        thetaTemp = exactSolution(Qs[i_batch], Rs[i_batch], thetaTemp, ys[i_batch], stepsize, n)\n",
        "        iterCount += 1\n",
        "        if iterCount % 50 == 0:\n",
        "            print(f'Splitting exactValues {exactValues(Xs, thetaTemp, ys)}, losses {losses[-1]:.5f}/{treshold:.5f}, iteration {iterCount}. Stepsize {stepsize}')\n",
        "        if losses[-1] <= treshold:\n",
        "            stopFlag = True\n",
        "            break\n",
        "        if losses[-1] >= 1e4 or iterCount >= maxIter:\n",
        "            stopFlag = True\n",
        "            iterCount = None\n",
        "    if iterCount == None:\n",
        "      print(f'\\n Splitting complete with fail. iteration {iterCount}, Stepsize {stepsize}')\n",
        "    else:\n",
        "      print(f'\\n Splitting complete with success. iteration {iterCount}, Stepsize {stepsize}')\n",
        "    return iterCount\n",
        "\n",
        "# Drawings helper\n",
        "def drawLearningRate2Time(learning_rates, list_of_methods, list_of_labels):\n",
        "    colors = ['g', 'r']\n",
        "    color_labels = ['^', 'o']\n",
        "    plt.figure(figsize = (3.5,2.5))\n",
        "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
        "        mean = np.zeros(len(learning_rates))\n",
        "        std = np.zeros(len(learning_rates))\n",
        "        for i_lr, lr in enumerate(learning_rates):\n",
        "            if any(method[:, i_lr]) == None:\n",
        "                mean[i_lr] = None\n",
        "                std[i_lr] = None\n",
        "            else:\n",
        "                mean[i_lr] = np.mean(method[:, i_lr])\n",
        "                std[i_lr] = np.std(method[:, i_lr])\n",
        "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
        "        plt.loglog(learning_rates, mean, color+':')\n",
        "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
        "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Time to converge')\n",
        "        plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Drawings helper\n",
        "def drawLearningRate2IterCount(learning_rates, list_of_methods, list_of_labels):\n",
        "    colors = ['g', 'r']\n",
        "    color_labels = ['^', 'o']\n",
        "    plt.figure(figsize = (3.5,2.5))\n",
        "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
        "        mean    = np.zeros(len(learning_rates))\n",
        "        std     = np.zeros(len(learning_rates))\n",
        "\n",
        "        for i_lr, lr in enumerate(learning_rates):\n",
        "            if any(method[:, i_lr]) == None:\n",
        "                mean[i_lr] = None\n",
        "                std[i_lr]  = None\n",
        "            else:\n",
        "                mean[i_lr] = np.mean(method[:, i_lr])\n",
        "                std[i_lr]  = np.std(method[:, i_lr])\n",
        "        std = np.std(method, axis = 0)   \n",
        "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
        "        plt.loglog(learning_rates, mean, color+':')\n",
        "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
        "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Iterations to converge')\n",
        "        plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print('Functions defined successfuly!')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Functions defined successfuly!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbcnvuHrtJS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for problem in problems:\n",
        "    print(f'Problem {problem} ')\n",
        "    if problem == 'tomography': # if data set is LLS, reassign parameters\n",
        "        X, theta_true, y = loadTomData()\n",
        "        n, p = X.shape\n",
        "        b, s = 60, 213\n",
        "\n",
        "    for i_exp in progress_bar(range(N_EXPERIMENTS)):\n",
        "        print(f'EXPERIMENT {i_exp + 1} / {N_EXPERIMENTS} ')\n",
        "        # Arranging dataset depend on problem\n",
        "        if problem == 'tomography':\n",
        "            init_bound = 1.0/math.sqrt(p)\n",
        "            theta_0 = np.array(init_bound*torch.FloatTensor(p).uniform_(-1, 1))\n",
        "            permutation = np.random.permutation(n)\n",
        "            X, y = X[permutation], y[permutation]\n",
        "        elif problem == 'random':\n",
        "            X, theta_0, y, theta_lstsq = generateRandomProblem(p,n, lstsq=True, eps=epsilon)\n",
        "\n",
        "        # Dividing batches\n",
        "        Xs = np.zeros((s, b, p))\n",
        "        ys = np.zeros((s, b))\n",
        "        Qs = np.zeros((s, p, b))\n",
        "        Rs = np.zeros((s, b, b))\n",
        "        Q, R = qr(X.T, mode='economic')\n",
        "        for i_batch in range(s):\n",
        "            Xs[i_batch] = X[b*i_batch:b*(i_batch+1), :]\n",
        "            ys[i_batch] = y[b*i_batch:b*(i_batch+1)]\n",
        "            Qs[i_batch], Rs[i_batch] = qr(Xs[i_batch].T, mode='economic')\n",
        "            \n",
        "        # Start to train and report\n",
        "        for i_lr, learning_rate in enumerate(LEARNING_RATES):\n",
        "            stepsize = learning_rate*n/b\n",
        "            print(f'learning_rate {learning_rate}, stepsize {stepsize} ')\n",
        "\n",
        "            start_time = time.time()\n",
        "            N_iter = trainSplitting(theta_0, Qs, Rs, Xs, ys, stepsize, treshold = TARGET_RES, maxIter = iter_limit)\n",
        "            end_time = time.time()\n",
        "            SPL_time[i_exp, i_lr] = end_time - start_time\n",
        "            SPL_N_iter[i_exp, i_lr] = N_iter\n",
        "            if N_iter == None:\n",
        "                SPL_N_iter[i_exp, i_lr] = None\n",
        "\n",
        "            start_time = time.time()\n",
        "            N_iter = trainSGD(theta_0, Xs, ys, learning_rate, treshold = TARGET_RES, maxIter = iter_limit)\n",
        "            end_time = time.time()\n",
        "            SGD_time[i_exp, i_lr] = end_time - start_time\n",
        "            SGD_N_iter[i_exp, i_lr] = N_iter\n",
        "            if N_iter == None:\n",
        "                SGD_time[i_exp, i_lr] = None\n",
        "\n",
        "            drawLearningRate2Time(LEARNING_RATES, [SPL_time, SGD_time], ['Splitting','SGD'])\n",
        "            drawLearningRate2IterCount(LEARNING_RATES, [SPL_N_iter, SGD_N_iter], ['Splitting','SGD'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSQWFEVEsv-e",
        "colab_type": "text"
      },
      "source": [
        "# Logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agz6M5zbsxpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_ERROR    = 1e-3 # desired loss rate\n",
        "N_EXPERIMENTS   = 30\n",
        "LEARNING_RATES  = np.array(np.logspace(-3, 2, 10))\n",
        "LEARNING_RATES  = np.append(LEARNING_RATES, np.array(np.logspace(2, 7, 6)))\n",
        "iter_limit      = 30000 # limit to iterations due to not converging\n",
        "GD_N_iter     = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SGD_N_iter    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SPL_N_iter    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "GD_time     = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SGD_time    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SPL_time    = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "batch_size = 50 # batch size\n",
        "number_of_classes = 2\n",
        "\n",
        "print('Problem parameters defined successfuly!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFOXvL2htMAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculates sigmoid funcion = 1/(1 + exp(-x_i))\n",
        "def sigmoid(x):\n",
        "    if np.isscalar(x):\n",
        "        return 1/(1 + np.exp(-x))\n",
        "    else:\n",
        "        return np.array([1/(1 + np.exp(-x_i)) for x_i in x])\n",
        "\n",
        "# Load data from CSV\n",
        "# Apply pruning to reshape\n",
        "# Create train and test set\n",
        "# Creating batches from train and test sets\n",
        "def load_batched_data_epi(batch_size=50, shuffle = True, qr_mode = False, number_of_classes = 2):\n",
        "    data = pd.read_csv('logreg/data.csv')\n",
        "    print(f'Before pruning {data.shape}')\n",
        "    data = data.dropna()\n",
        "    y = data['y']\n",
        "    X = data.drop(data.columns[0], axis=1)\n",
        "    X = X.drop(columns=['y'])\n",
        "    select_binary = (y == 2) + (y == 1)\n",
        "    X, y = X[select_binary], y[select_binary]\n",
        "    print(f'After pruning {data.shape}, {X.shape}, {y.shape}')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "    n_train, p = X_train.shape\n",
        "    n_test = len(y_test)\n",
        "    s_train = int(n_train/batch_size)   \n",
        "    K = number_of_classes \n",
        "    X_trains = torch.zeros((s_train, batch_size, p), requires_grad=False).to(device)\n",
        "    y_trains = torch.zeros((s_train, batch_size), requires_grad=False).to(device)\n",
        "    if qr_mode:\n",
        "        Qs = torch.zeros((s_train, p, batch_size), requires_grad=False).to(device)\n",
        "        Rs = torch.zeros((s_train, batch_size, batch_size), requires_grad=False).to(device)\n",
        "    for i in range(s_train):\n",
        "        X_trains[i] = torch.from_numpy(X_train[batch_size*i:batch_size*(i+1)].to_numpy())\n",
        "        y_trains[i] = torch.from_numpy(y_train[batch_size*i:batch_size*(i+1)].to_numpy())\n",
        "        if qr_mode:\n",
        "            Qs[i], Rs[i] = torch.qr(X_trains[i].t())      \n",
        "    print(type(X_trains), type(y_trains), type(X_test), type(y_test), type(Qs), type(Rs))        \n",
        "    if qr_mode:\n",
        "        return X_trains, y_trains, torch.from_numpy(X_test.to_numpy()), torch.from_numpy(y_test.to_numpy()), Qs, Rs\n",
        "    else:\n",
        "        return X_trains, y_trains, torch.from_numpy(X_test.to_numpy()), torch.from_numpy(y_test.to_numpy())\n",
        "\n",
        "\n",
        "# Load data from MNIST\n",
        "# Create train and test set\n",
        "# Apply shuffling to handle memorizing\n",
        "# Creating batches from train and test sets\n",
        "def load_batched_data(batch_size=50, shuffle = True, qr_mode = False, number_of_classes = 2):\n",
        "    trainset = datasets.MNIST('./mnist_data/', download=True, train=True)\n",
        "    X_train = trainset.train_data.to(dtype=torch.float)/255\n",
        "    y_train = trainset.train_labels\n",
        "    mask = y_train < number_of_classes\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    X_train.resize_(len(X_train), *X_train[0].view(-1).shape)\n",
        "    y_train.view(-1).long()\n",
        "    testset = datasets.MNIST('./mnist_data/', download=True, train=False)\n",
        "    X_test = testset.test_data.to(dtype=torch.float)/255\n",
        "    y_test = testset.test_labels\n",
        "    mask = y_test < number_of_classes\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    X_test.resize_(len(X_test), *X_test[0].view(-1).shape)\n",
        "    y_test.view(-1).long()\n",
        "    if shuffle == True:\n",
        "        shuffling = torch.randperm(len(y_train))\n",
        "        X_train = X_train[shuffling]\n",
        "        y_train = y_train[shuffling]\n",
        "    if shuffle == True:\n",
        "        shuffling = torch.randperm(len(y_test))\n",
        "        X_test = X_test[shuffling].to(device)\n",
        "        y_test = y_test[shuffling]\n",
        "    n_train = len(y_train)\n",
        "    n_test  = len(y_test)\n",
        "    s_train = int(n_train/batch_size)  \n",
        "    K = number_of_classes \n",
        "    X_trains = torch.zeros((s_train, batch_size, *X_train[0].view(-1).shape), requires_grad=False).to(device)\n",
        "    y_trains = torch.zeros((s_train, batch_size), requires_grad=False).to(device)\n",
        "    if qr_mode:\n",
        "        Qs = torch.zeros((s_train, *X_train[0].view(-1).shape, batch_size), requires_grad=False).to(device)\n",
        "        Rs = torch.zeros((s_train, batch_size, batch_size), requires_grad=False).to(device)\n",
        "    for i in range(s_train):\n",
        "        X_trains[i] = X_train[batch_size*i:batch_size*(i+1), :]\n",
        "        y_trains[i] = y_train[batch_size*i:batch_size*(i+1)]\n",
        "        if qr_mode:\n",
        "            Qs[i], Rs[i] = torch.qr(X_trains[i].t())      \n",
        "    if qr_mode:\n",
        "        return X_trains, y_trains, X_test, y_test, Qs, Rs\n",
        "    else:\n",
        "        return X_trains, y_trains, X_test, y_test\n",
        "\n",
        "# Creating Logistic Regression Model\n",
        "# Adding one Linear Layer\n",
        "# using sigmoid activation function to make prediction\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "     def __init__(self):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(p, 1)\n",
        "     def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "# Merge dataset batches to single array\n",
        "def full_problem_from_batches(Xs, ys):\n",
        "    s_train, batch_size, p = Xs.shape\n",
        "    X = torch.zeros(s_train*batch_size, p)\n",
        "    y = torch.zeros(s_train*batch_size)\n",
        "    for i_batch in range(s_train):\n",
        "        X[batch_size*i_batch:batch_size*(i_batch+1), :] = Xs[i_batch]\n",
        "        y[batch_size*i_batch:batch_size*(i_batch+1)]    = ys[i_batch]\n",
        "    return X, y\n",
        "\n",
        "# Creating new model instance\n",
        "# We won't update bias during the training, since they are not affect the model predictions\n",
        "def model_init(model, parameters_tensor):\n",
        "    new_model = copy.deepcopy(model)\n",
        "    for parameter in new_model.parameters():\n",
        "        parameter.data = parameters_tensor.clone().to(device)\n",
        "        break\n",
        "    return new_model\n",
        "\n",
        "# train SGD \n",
        "def sgd_training(theta_0, X_trains, y_trains,  X_test, y_test, lr, model, final_error = 0.2, iter_limit = 1000):\n",
        "    X, y = full_problem_from_batches(X_trains, y_trains)\n",
        "    X, y, X_test, y_test = X.float().to(device), y.float().to(device), X_test.to(device), y_test.to(device)\n",
        "    model = model.to(device)\n",
        "    s_train, batch_size, p = X_trains.shape\n",
        "    n_train, p  = X.shape\n",
        "    n_test = len(y_test)\n",
        "    thetas = []\n",
        "    losses_train = []\n",
        "    errors_train = []\n",
        "    losses_test = []\n",
        "    errors_test = []\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    theta_t = theta_0\n",
        "    model = model_init(model, theta_0.t())\n",
        "    stop_word = False\n",
        "    N_iter = 0\n",
        "    if lr >= 0.2:\n",
        "        iter_limit = 1000\n",
        "    while not stop_word:          \n",
        "        i_batch = N_iter % s_train\n",
        "        if i_batch % 1 == 0:\n",
        "            model.eval()\n",
        "            y_pred = model(X)\n",
        "            loss = criterion(y_pred, y)\n",
        "            thetas.append(theta_t)\n",
        "            losses_train.append(loss.data)\n",
        "            pred_labels     = torch.squeeze(y_pred >= 0.5).float()\n",
        "            train_acc       = y.eq(pred_labels.data).sum().to(dtype=torch.float)/len(pred_labels)\n",
        "            errors_train.append(1 - train_acc) \n",
        "            y_pred_test = model(X_test)\n",
        "            loss_test   = criterion(y_pred_test, y_test.float())\n",
        "            losses_test.append(loss_test.data)\n",
        "            pred_labels_test    = torch.squeeze(y_pred_test >= 0.5).long()\n",
        "            test_acc            = y_test.eq(pred_labels_test.data).sum().to(dtype=torch.float)/len(y_pred_test)\n",
        "            errors_test.append(1 - test_acc)\n",
        "            if errors_test[-1] <= final_error:\n",
        "                stop_word = True\n",
        "                break\n",
        "            if N_iter >= iter_limit:\n",
        "                N_iter = None\n",
        "                print(f'\\nðŸ¤– SGD Failed on lr {lr}')\n",
        "                return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        y_pred = model(X_trains[i_batch])\n",
        "        loss = criterion(y_pred, y_trains[i_batch])\n",
        "        loss.backward()\n",
        "        for parameter in model.parameters():\n",
        "            parameter.data = parameter.data - lr*parameter.grad.data\n",
        "            theta_t = np.array((parameter.data.t()).cpu())\n",
        "            break\n",
        "        N_iter += 1\n",
        "    print(f'SGD finished with {N_iter} iterations on lr {lr}')\n",
        "    return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
        "\n",
        "# C.2.2 function 27 \n",
        "# SPLITTING SCHEME\n",
        "# integrate from 0 to h\n",
        "def make_splitting_step(theta_0, Q, R, y, h, n):\n",
        "    h_seq = [0, h]\n",
        "    Q, R, theta_0 = np.array(Q), np.array(R), np.array(theta_0)\n",
        "    eta_0, theta_0 = np.squeeze(Q.T@theta_0), np.squeeze(theta_0)\n",
        "    def rhs(eta, t):\n",
        "        return -1/n * R@(sigmoid(R.T @ eta) - np.array(y))\n",
        "    eta_h = odeint(rhs, eta_0, h_seq)[-1]\n",
        "    theta = Q@(eta_h - eta_0) + theta_0\n",
        "    return torch.from_numpy(theta).reshape(p, 1)\n",
        "\n",
        "# train splitting \n",
        "def spl_training(theta_0, Qs, Rs, X_trains, y_trains,  X_test, y_test, stepsize, model, final_error = 0.2, iter_limit = 1000):\n",
        "    X, y = full_problem_from_batches(X_trains, y_trains)\n",
        "    X, y, X_trains, y_trains, X_test, y_test, model = X.float().to(device), y.float().to(device), X_trains.float().to(device), y_trains.float().to(device), X_test.float().to(device), y_test.float().to(device), model.to(device)\n",
        "    s_train, batch_size, p = X_trains.shape\n",
        "    n_train, p  = X.shape\n",
        "    n_test = len(y_test)\n",
        "    thetas = []\n",
        "    losses_train = []\n",
        "    errors_train = []\n",
        "    losses_test = []\n",
        "    errors_test = []\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    theta_t = theta_0.to(device)\n",
        "    model = model_init(model, theta_0.t())\n",
        "    stop_word = False\n",
        "    N_iter = 0\n",
        "    if stepsize >= 1000:\n",
        "        iter_limit = 1000\n",
        "    while not stop_word:\n",
        "        i_batch = N_iter % s_train\n",
        "        if i_batch % 1 == 0:      \n",
        "            model.eval()\n",
        "            y_pred = model(X)\n",
        "            loss = criterion(y_pred, y)\n",
        "            thetas.append(theta_t)\n",
        "            losses_train.append(loss.data)\n",
        "            pred_labels = torch.squeeze(y_pred >= 0.5).float()\n",
        "            train_acc = y.eq(pred_labels.data).sum().to(dtype=torch.float)/len(pred_labels)\n",
        "            errors_train.append(1 - train_acc) \n",
        "            y_pred_test = model(X_test)\n",
        "            loss_test = criterion(y_pred_test, y_test)\n",
        "            losses_test.append(loss_test.data)\n",
        "            pred_labels_test = torch.squeeze(y_pred_test >= 0.5).float()\n",
        "            test_acc = y_test.eq(pred_labels_test.data).sum().to(dtype=torch.float)/len(y_pred_test)\n",
        "            errors_test.append(1 - test_acc)\n",
        "            if errors_test[-1] <= final_error:\n",
        "                stop_word = True\n",
        "                break\n",
        "            if N_iter >= iter_limit:\n",
        "                N_iter = None\n",
        "                print(f'\\n Splitting Failed on lr {lr}')\n",
        "                return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
        "        model.train()\n",
        "        theta_t = make_splitting_step(theta_t.cpu(), Qs[i_batch].cpu(), Rs[i_batch].cpu(), y_trains[i_batch].cpu(), stepsize, n_train).to(dtype=torch.float)\n",
        "        model = model_init(model, theta_t.t())\n",
        "        N_iter += 1  \n",
        "    print(f' Splitting finished with {N_iter} iterations on Stepsize {stepsize}')\n",
        "    return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
        "\n",
        "# Drawings helper\n",
        "def drawLearningRateTime(learning_rates, list_of_methods, list_of_labels):\n",
        "    colors = ['g', 'r']\n",
        "    color_labels = ['^', 'o']\n",
        "    plt.figure(figsize = (3.5,2.5))\n",
        "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
        "        mean = np.zeros(len(learning_rates))\n",
        "        std = np.zeros(len(learning_rates))\n",
        "        for i_lr, lr in enumerate(learning_rates):\n",
        "            if any(method[:, i_lr]) == None:\n",
        "                mean[i_lr] = None\n",
        "                std[i_lr]  = None\n",
        "            else:\n",
        "                mean[i_lr] = np.mean(method[:, i_lr])\n",
        "                std[i_lr]  = np.std(method[:, i_lr])\n",
        "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
        "        plt.loglog(learning_rates, mean, color+':')\n",
        "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
        "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Time to converge')\n",
        "        plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Drawings helper\n",
        "def drawLearningRateIterCount(learning_rates, list_of_methods, list_of_labels):\n",
        "    colors = ['g', 'r']\n",
        "    color_labels = ['^', 'o']\n",
        "    plt.figure(figsize = (3.5,2.5))\n",
        "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
        "        mean = np.zeros(len(learning_rates))\n",
        "        std = np.zeros(len(learning_rates))\n",
        "        for i_lr, lr in enumerate(learning_rates):\n",
        "            if any(method[:, i_lr]) == None:\n",
        "                mean[i_lr] = None\n",
        "                std[i_lr]  = None\n",
        "            else:\n",
        "                mean[i_lr] = np.mean(method[:, i_lr])\n",
        "                std[i_lr]  = np.std(method[:, i_lr])\n",
        "        std = np.std(method, axis = 0)   \n",
        "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
        "        plt.loglog(learning_rates, mean, color+':')\n",
        "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
        "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Iterations to converge')\n",
        "        plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print('Functions defined Successfuly!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMXbYlzOtO2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assigning dataset, batches and start to train\n",
        "X_trains, y_trains, X_test, y_test, Qs, Rs = load_batched_data_epi(batch_size=batch_size, qr_mode = True, number_of_classes=number_of_classes)\n",
        "s_train, batch_size, p = X_trains.shape \n",
        "n_train, n_test = s_train*batch_size, len(y_test)\n",
        "model = LogisticRegression()\n",
        "for i_exp in progress_bar(range(N_EXPERIMENTS)):\n",
        "    print(f'EXPERIMENT {i_exp+1}/ {N_EXPERIMENTS} ')\n",
        "    init_bound = 1.0/math.sqrt(p)\n",
        "    theta_0 = init_bound*torch.FloatTensor(p, 1).uniform_(-1, 1)\n",
        "    for i_lr, learning_rate in enumerate(LEARNING_RATES):\n",
        "        stepsize = learning_rate*n_train/batch_size\n",
        "        print(f'learning_rate {learning_rate}, stepsize {stepsize} ')\n",
        "        start_time = time.time()\n",
        "        N_iter, thetas, losses_train,losses_test, errors_train, errors_test = \\\n",
        "            spl_training(theta_0,  Qs, Rs, X_trains, y_trains,  X_test, y_test, stepsize, model, final_error = TARGET_ERROR, iter_limit=iter_limit)\n",
        "        end_time = time.time()\n",
        "        SPL_time[i_exp, i_lr] = end_time - start_time\n",
        "        SPL_N_iter[i_exp, i_lr] = N_iter\n",
        "        if N_iter == None:\n",
        "            SPL_N_iter[i_exp, i_lr] = None\n",
        "        start_time = time.time()\n",
        "        N_iter, thetas, losses_train,losses_test, errors_train, errors_test = \\\n",
        "            sgd_training(theta_0, X_trains, y_trains,  X_test, y_test, learning_rate, model, final_error = TARGET_ERROR, iter_limit=iter_limit)\n",
        "        end_time = time.time()\n",
        "        SGD_time[i_exp, i_lr] = end_time - start_time\n",
        "        SGD_N_iter[i_exp, i_lr] = N_iter\n",
        "        if N_iter == None:\n",
        "            SGD_time[i_exp, i_lr] = None\n",
        "\n",
        "        drawLearningRateTime(LEARNING_RATES, [SPL_time, SGD_time], ['Splitting','SGD'])\n",
        "        drawLearningRateIterCount(LEARNING_RATES, [SPL_N_iter, SGD_N_iter], ['Splitting','SGD'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcYGpJDvs0Iq",
        "colab_type": "text"
      },
      "source": [
        "# Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCpDiBdlszi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_ERROR = 0.25 # desired loss rate\n",
        "N_EXPERIMENTS = 10\n",
        "LEARNING_RATES = np.array(np.logspace(-2, 1.5, 10))[::-1]\n",
        "iter_limit = 300000#  limit to iterations due to not converging\n",
        "GD_time = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SGD_time = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SPL_time = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "GD_N_iter = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SGD_N_iter = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "SPL_N_iter = np.zeros((N_EXPERIMENTS, len(LEARNING_RATES)))\n",
        "batch_size = 64 # batch size\n",
        "number_of_classes = 10\n",
        "\n",
        "print('Problem parameters defined successfuly!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iYcE30btRo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data from MNIST\n",
        "# Create train and test set\n",
        "# Apply shuffling to handle memorizing\n",
        "# Creating batches from train and test sets\n",
        "def load_batched_data(batch_size=50, shuffle = True, qr_mode = False, number_of_classes = 3):\n",
        "    trainset = datasets.FashionMNIST('./fashion_mnist_data/', download=True, train=True)\n",
        "    X_train = trainset.train_data.to(dtype=torch.float)/255\n",
        "    y_train = trainset.train_labels\n",
        "    mask = y_train < number_of_classes\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    X_train.resize_(len(X_train), *X_train[0].view(-1).shape)\n",
        "    y_train.view(-1).long()\n",
        "    testset = datasets.FashionMNIST('./fashion_mnist_data/', download=True, train=False)\n",
        "    X_test = testset.test_data.to(dtype=torch.float)/255\n",
        "    y_test = testset.test_labels\n",
        "    mask = y_test < number_of_classes\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    X_test.resize_(len(X_test), *X_test[0].view(-1).shape)\n",
        "    y_test.view(-1).long()\n",
        "    if shuffle == True:\n",
        "        shuffling = torch.randperm(len(y_train))\n",
        "        X_train = X_train[shuffling]\n",
        "        y_train = y_train[shuffling]\n",
        "    if shuffle == True:\n",
        "        shuffling = torch.randperm(len(y_test))\n",
        "        X_test = X_test[shuffling].to(device)\n",
        "        y_test = y_test[shuffling]\n",
        "    n_train = len(y_train)\n",
        "    n_test = len(y_test)\n",
        "    s_train = int(n_train/batch_size)   \n",
        "    K = number_of_classes \n",
        "    X_trains = torch.zeros((s_train, batch_size, *X_train[0].view(-1).shape), requires_grad=False).to(device)\n",
        "    y_trains = torch.zeros((s_train, K, batch_size), requires_grad=False, dtype=torch.int64).to(device)\n",
        "    if qr_mode:\n",
        "        Qs = torch.zeros((s_train, *X_train[0].view(-1).shape, batch_size), requires_grad=False).to(device)\n",
        "        Rs = torch.zeros((s_train, batch_size, batch_size), requires_grad=False).to(device)\n",
        "    y_test_one_hot = torch.zeros((n_test, K))\n",
        "    y_test_one_hot[np.arange(n_test), y_test] = 1\n",
        "    y_test_one_hot = y_test_one_hot.t()\n",
        "    for i in range(s_train):\n",
        "        X_trains[i] = X_train[batch_size*i:batch_size*(i+1), :]\n",
        "        batch_lbls  = y_train[batch_size*i:batch_size*(i+1)]\n",
        "        y_batch_one_hot = torch.zeros((batch_size, K))\n",
        "        y_batch_one_hot[np.arange(batch_size), batch_lbls] = 1\n",
        "        y_trains[i] = y_batch_one_hot.t()\n",
        "        if qr_mode:\n",
        "            Qs[i], Rs[i] = torch.qr(X_trains[i].t())      \n",
        "    if qr_mode:\n",
        "        return X_trains, y_trains, X_test, y_test_one_hot, Qs, Rs\n",
        "    else:\n",
        "        return X_trains, y_trains, X_test, y_test_one_hot\n",
        "\n",
        "# Load data from CIFAR\n",
        "# Create train and test set\n",
        "# Apply shuffling to handle memorizing\n",
        "# Creating batches from train and test sets\n",
        "def load_batched_data_cifar(batch_size=50, shuffle = True, qr_mode = False, number_of_classes = 3):\n",
        "    trainset = datasets.CIFAR10('./cifar_data/', download=True, train=True)\n",
        "    X_train = torch.from_numpy(trainset.train_data).to(dtype=torch.float)/255\n",
        "    y_train = torch.Tensor(trainset.train_labels)\n",
        "    mask = y_train < number_of_classes\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    X_train.resize_(len(X_train), *X_train[0].view(-1).shape)\n",
        "    y_train.view(-1).long()\n",
        "    testset = datasets.CIFAR10('./cifar_data/', download=True, train=False)\n",
        "    X_test = torch.from_numpy(testset.test_data).to(dtype=torch.float)/255\n",
        "    y_test = torch.Tensor(testset.test_labels)\n",
        "    mask = y_test < number_of_classes\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    X_test.resize_(len(X_test), *X_test[0].view(-1).shape)\n",
        "    y_test.view(-1).long()\n",
        "    if shuffle == True:\n",
        "        shuffling = torch.randperm(len(y_train))\n",
        "        X_train = X_train[shuffling]\n",
        "        y_train = y_train[shuffling]\n",
        "    if shuffle == True:\n",
        "        shuffling = torch.randperm(len(y_test))\n",
        "        X_test = X_test[shuffling].to(device)\n",
        "        y_test = y_test[shuffling]\n",
        "    n_train = len(y_train)\n",
        "    n_test = len(y_test)\n",
        "    s_train = int(n_train/batch_size) \n",
        "    K = number_of_classes \n",
        "    X_trains = torch.zeros((s_train, batch_size, *X_train[0].view(-1).shape), requires_grad=False).to(device)\n",
        "    y_trains = torch.zeros((s_train, K, batch_size), requires_grad=False, dtype=torch.int64).to(device)\n",
        "    if qr_mode:\n",
        "        Qs = torch.zeros((s_train, *X_train[0].view(-1).shape, batch_size), requires_grad=False).to(device)\n",
        "        Rs = torch.zeros((s_train, batch_size, batch_size), requires_grad=False).to(device)\n",
        "    y_test_one_hot = torch.zeros((n_test, K))\n",
        "    y_test_one_hot[np.arange(n_test), y_test.long()] = 1\n",
        "    y_test_one_hot = y_test_one_hot.t()\n",
        "    for i in range(s_train):\n",
        "        X_trains[i] = X_train[batch_size*i:batch_size*(i+1), :]\n",
        "        batch_lbls = y_train[batch_size*i:batch_size*(i+1)]\n",
        "        y_batch_one_hot = torch.zeros((batch_size, K))\n",
        "        y_batch_one_hot[np.arange(batch_size), batch_lbls.long()] = 1\n",
        "        y_trains[i] = y_batch_one_hot.t()\n",
        "        if qr_mode:\n",
        "            Qs[i], Rs[i] = torch.qr(X_trains[i].t())      \n",
        "    if qr_mode:\n",
        "        return X_trains, y_trains, X_test, y_test_one_hot, Qs, Rs\n",
        "    else:\n",
        "        return X_trains, y_trains, X_test, y_test_one_hot\n",
        "\n",
        "# C.3.1 function 31 and 32 softmax function \n",
        "def softmax_numpy(X):\n",
        "    return np.array([np.exp(x)/sum(np.exp(x)) for x in X.T]).T\n",
        "\n",
        "# Creating CrossEntropyLoss model\n",
        "# Modify to compute one-hot encoding\n",
        "class CrossEntropyLoss_one_hot(nn.CrossEntropyLoss):\n",
        "    def forward(self, input, target):\n",
        "        target = torch.squeeze(torch.max(target, 1, keepdim=True)[1])\n",
        "        return F.cross_entropy(input, target, weight=self.weight,\n",
        "                                ignore_index=self.ignore_index, reduction=self.reduction)\n",
        "\n",
        "# Merge dataset batches to single array\n",
        "def full_problem_from_batches(Xs, ys):\n",
        "    s_train, batch_size, p = Xs.shape\n",
        "    s_train, K, batch_size = ys.shape\n",
        "    X = torch.zeros(s_train*batch_size, p)\n",
        "    y = torch.zeros(K, s_train*batch_size)\n",
        "    for i_batch in range(s_train):\n",
        "        X[batch_size*i_batch:batch_size*(i_batch+1), :] = Xs[i_batch]\n",
        "        y[:, batch_size*i_batch:batch_size*(i_batch+1)] = ys[i_batch]\n",
        "    return X, y\n",
        "\n",
        "# Creating new model instance\n",
        "# We won't update bias during the training, since they are not affect the model predictions\n",
        "def model_init(model, parameters_tensor):\n",
        "    new_model = copy.deepcopy(model)\n",
        "    for parameter in new_model.parameters():\n",
        "        parameter.data = parameters_tensor.clone().to(device)\n",
        "        break\n",
        "    return new_model\n",
        "\n",
        "# Creating Logistic Regression Model\n",
        "# Adding one Linear Layer\n",
        "# using softmax activation function to make prediction\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(int(input_dim), int(output_dim))\n",
        "    def forward(self, x):\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        return F.softmax(self.linear(x), dim=1)\n",
        "\n",
        "# Creating new LogisticRegression instance\n",
        "def load_model(X_test, y_test):\n",
        "    input_dim = X_test[0].numel()\n",
        "    K, n_test = y_test.shape\n",
        "    model = LogisticRegression(input_dim, K)\n",
        "    return model\n",
        "\n",
        "# train SGD\n",
        "def sgd_training(theta_0, X_trains, y_trains,  X_test, y_test, lr, model, final_error = 0.2, iter_limit = 1000):\n",
        "    X, y = full_problem_from_batches(X_trains, y_trains)\n",
        "    X, y, X_test, y_test = X.float().to(device), y.float().to(device), X_test.to(device), y_test.to(device)\n",
        "    model = model.to(device)\n",
        "    s_train, batch_size, p = X_trains.shape\n",
        "    n_train, p  = X.shape\n",
        "    K, n_test = y_test.shape\n",
        "    thetas = []\n",
        "    losses_train = []\n",
        "    errors_train = []\n",
        "    losses_test = []\n",
        "    errors_test = []\n",
        "    criterion = CrossEntropyLoss_one_hot()\n",
        "    theta_t = theta_0\n",
        "    model = model_init(model, theta_0.t())\n",
        "    stop_word = False\n",
        "    N_iter = 0\n",
        "    if lr >= 1:\n",
        "        iter_limit = 30000\n",
        "    if lr >= 10:\n",
        "        iter_limit = 1000\n",
        "    while not stop_word:          \n",
        "        i_batch = N_iter % s_train\n",
        "        if i_batch % 1 == 0:\n",
        "            model.eval()\n",
        "            y_pred = model(X)\n",
        "            loss = criterion(y_pred, y.t())\n",
        "            thetas.append(theta_t)\n",
        "            losses_train.append(loss.data)\n",
        "            pred_labels = torch.max(y_pred, 1, keepdim=True)[1]\n",
        "            true_labels = torch.max(y.t(), 1, keepdim=True)[1]\n",
        "            train_acc = true_labels.eq(pred_labels.data).sum().to(dtype=torch.float)/len(true_labels)\n",
        "            errors_train.append(1 - train_acc) \n",
        "            y_pred_test = model(X_test)\n",
        "            loss_test = criterion(y_pred_test, y_test.t())\n",
        "            losses_test.append(loss_test.data)\n",
        "            pred_labels_test = torch.max(y_pred_test, 1, keepdim=True)[1]\n",
        "            true_labels_test = torch.max(y_test.t(), 1, keepdim=True)[1]\n",
        "            test_acc = true_labels_test.eq(pred_labels_test.data).sum().to(dtype=torch.float)/len(true_labels_test)\n",
        "            errors_test.append(1 - test_acc)\n",
        "            if errors_test[-1] <= final_error:\n",
        "                stop_word = True\n",
        "                break\n",
        "            if N_iter >= iter_limit:\n",
        "                N_iter = None\n",
        "                return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
        "        model.train()\n",
        "        model.zero_grad()\n",
        "        y_pred = model(X_trains[i_batch])\n",
        "        loss = criterion(y_pred, y_trains[i_batch].t())\n",
        "        loss.backward()\n",
        "        for parameter in model.parameters():\n",
        "            parameter.data = parameter.data - lr*parameter.grad.data\n",
        "            theta_t = np.array((parameter.data.t()).cpu())\n",
        "            break\n",
        "        N_iter += 1\n",
        "    return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
        "# C.3.1 Final formula to calculate one step of splitting\n",
        "def make_splitting_step(theta_0, Q, R, y, h, n_train):\n",
        "    p, K = theta_0.shape\n",
        "    p, batch_size = Q.shape\n",
        "    Q, R, y = np.array(Q), np.array(R), np.array(y)\n",
        "    h_seq = [0, h]\n",
        "    theta_0 = np.array(theta_0)   \n",
        "    H_0 = np.array(Q.T @ theta_0)\n",
        "    H_0_vec = H_0.ravel('F')\n",
        "    H_h = np.zeros((batch_size, K))\n",
        "    def rhs_vec(H, t):\n",
        "        H = H.reshape((batch_size, K), order='F')\n",
        "        rhs = -1/n_train * R@(softmax_numpy(H.T@R) - y).T\n",
        "        return rhs.ravel('F')\n",
        "    H_h_vec = odeint(rhs_vec, H_0_vec, h_seq)[-1]\n",
        "    H_h = H_h_vec.reshape((batch_size, K), order='F')\n",
        "    theta = Q@(H_h - H_0) + theta_0\n",
        "    return torch.from_numpy(theta)\n",
        "\n",
        "# Train Splitting\n",
        "def spl_training(theta_0, Qs, Rs, X_trains, y_trains,  X_test, y_test, stepsize, model, final_error = 0.2, iter_limit = 1000):\n",
        "    X, y = full_problem_from_batches(X_trains, y_trains)\n",
        "    X, y, X_trains, y_trains, X_test, y_test, model = X.float().to(device), y.float().to(device), X_trains.float().to(device), y_trains.float().to(device), X_test.float().to(device), y_test.float().to(device), model.to(device)\n",
        "    s_train, batch_size, p = X_trains.shape\n",
        "    n_train, p  = X.shape\n",
        "    K, n_test = y_test.shape\n",
        "    thetas = []\n",
        "    losses_train = []\n",
        "    errors_train = []\n",
        "    losses_test = []\n",
        "    errors_test = []\n",
        "    criterion = CrossEntropyLoss_one_hot()\n",
        "    theta_t = theta_0.to(device)\n",
        "    model = model_init(model, theta_0.t())\n",
        "    stop_word = False\n",
        "    N_iter = 0\n",
        "    if stepsize >= 1000:\n",
        "        iter_limit = 1000\n",
        "    while not stop_word:\n",
        "        i_batch = N_iter % s_train\n",
        "        if i_batch % 1 == 0:      \n",
        "            model.eval()\n",
        "            y_pred = model(X)\n",
        "            loss = criterion(y_pred, y.t())\n",
        "            thetas.append(theta_t)\n",
        "            losses_train.append(loss.data)\n",
        "            pred_labels = torch.max(y_pred, 1, keepdim=True)[1]\n",
        "            true_labels = torch.max(y.t(), 1, keepdim=True)[1]\n",
        "            train_acc = true_labels.eq(pred_labels.data).sum().to(dtype=torch.float)/len(true_labels)\n",
        "            errors_train.append(1 - train_acc) \n",
        "            y_pred_test = model(X_test)\n",
        "            loss_test = criterion(y_pred_test, y_test.t())\n",
        "            losses_test.append(loss_test.data)\n",
        "            pred_labels_test = torch.max(y_pred_test, 1, keepdim=True)[1]\n",
        "            true_labels_test = torch.max(y_test.t(), 1, keepdim=True)[1]\n",
        "            test_acc = true_labels_test.eq(pred_labels_test.data).sum().to(dtype=torch.float)/len(true_labels_test)\n",
        "            errors_test.append(1 - test_acc)\n",
        "            if errors_test[-1] <= final_error:\n",
        "                stop_word = True\n",
        "                break\n",
        "            if N_iter >= iter_limit:\n",
        "                N_iter = None\n",
        "                return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
        "        model.train()\n",
        "        theta_t = make_splitting_step(theta_t.cpu(), Qs[i_batch].cpu(), Rs[i_batch].cpu(), y_trains[i_batch].cpu(), stepsize, n_train).to(dtype=torch.float)\n",
        "        model = model_init(model, theta_t.t())\n",
        "        N_iter += 1  \n",
        "    return N_iter, thetas, losses_train,losses_test, errors_train, errors_test\n",
        "\n",
        "# Drawing Helper\n",
        "def drawLearningRateTime(learning_rates, list_of_methods, list_of_labels):\n",
        "    colors = ['g', 'r']\n",
        "    color_labels = ['^', 'o']\n",
        "    plt.figure(figsize = (3.5,2.5))\n",
        "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
        "        mean = np.zeros(len(learning_rates))\n",
        "        std = np.zeros(len(learning_rates))\n",
        "        for i_lr, lr in enumerate(learning_rates):\n",
        "            if any(method[:, i_lr]) == None:\n",
        "                mean[i_lr] = None\n",
        "                std[i_lr]  = None\n",
        "            else:\n",
        "                mean[i_lr] = np.mean(method[:, i_lr])\n",
        "                std[i_lr]  = np.std(method[:, i_lr])\n",
        "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
        "        plt.loglog(learning_rates, mean, color+':')\n",
        "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
        "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Time to converge')\n",
        "        plt.legend()\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Drawing Helper\n",
        "def drawLearningRateIterCount(learning_rates, list_of_methods, list_of_labels):\n",
        "    colors = ['g', 'r']\n",
        "    color_labels = ['^', 'o']\n",
        "    plt.figure(figsize = (3.5,2.5))\n",
        "    for method, label, color, col_lab in zip(list_of_methods, list_of_labels, colors, color_labels):\n",
        "        mean = np.zeros(len(learning_rates))\n",
        "        std = np.zeros(len(learning_rates))\n",
        "        for i_lr, lr in enumerate(learning_rates):\n",
        "            if any(method[:, i_lr]) == None:\n",
        "                mean[i_lr] = None\n",
        "                std[i_lr]  = None\n",
        "            else:\n",
        "                mean[i_lr] = np.mean(method[:, i_lr])\n",
        "                std[i_lr]  = np.std(method[:, i_lr])\n",
        "        std = np.std(method, axis = 0)   \n",
        "        plt.loglog(learning_rates, mean, color+col_lab, label = label)\n",
        "        plt.loglog(learning_rates, mean, color+':')\n",
        "        plt.fill_between(learning_rates, mean-std, mean+std, color=color, alpha=0.1)\n",
        "        plt.grid(True,which=\"both\", linestyle='--', linewidth=0.4)\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Iterations to converge')\n",
        "        plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "print('Functions defined Successfuly!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvd0uIeGtR6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_trains, y_trains, X_test, y_test, Qs, Rs = load_batched_data(batch_size=batch_size, qr_mode = True, number_of_classes=number_of_classes)\n",
        "s_train, batch_size, p = X_trains.shape \n",
        "n_train, K, n_test = s_train*batch_size, *y_test.shape\n",
        "model = load_model(X_test, y_test)\n",
        "for i_exp in progress_bar(range(N_EXPERIMENTS)):\n",
        "    print(f' EXPERIMENTS {i_exp+1}/ {N_EXPERIMENTS} ')\n",
        "    init_bound = 1.0/math.sqrt(p)\n",
        "    theta_0 = init_bound*torch.FloatTensor(p, K).uniform_(-1, 1)\n",
        "    for i_lr, learning_rate in enumerate(LEARNING_RATES):\n",
        "        stepsize = learning_rate*n_train/batch_size\n",
        "        print(f'learning_rate {learning_rate}, stepsize {stepsize} ')\n",
        "        start_time = time.time()\n",
        "        N_iter, thetas, losses_train,losses_test, errors_train, errors_test = \\\n",
        "            spl_training(theta_0,  Qs, Rs, X_trains, y_trains,  X_test, y_test, stepsize, model, final_error = TARGET_ERROR, iter_limit=iter_limit)\n",
        "        end_time = time.time()\n",
        "        SPL_time[i_exp, i_lr] = end_time - start_time\n",
        "        SPL_N_iter[i_exp, i_lr] = N_iter\n",
        "        if N_iter == None:\n",
        "            SPL_N_iter[i_exp, i_lr] = None\n",
        "        start_time = time.time()\n",
        "        N_iter, thetas, losses_train,losses_test, errors_train, errors_test = \\\n",
        "            sgd_training(theta_0, X_trains, y_trains,  X_test, y_test, learning_rate, model, final_error = TARGET_ERROR, iter_limit=iter_limit)\n",
        "        end_time = time.time()\n",
        "        SGD_time[i_exp, i_lr] = end_time - start_time\n",
        "        SGD_N_iter[i_exp, i_lr] = N_iter\n",
        "        if N_iter == None:\n",
        "            SGD_time[i_exp, i_lr] = None\n",
        "        drawLearningRateTime(LEARNING_RATES, [SPL_time, SGD_time], ['Splitting','SGD'])\n",
        "        drawLearningRateIterCount(LEARNING_RATES, [SPL_N_iter, SGD_N_iter], ['Splitting','SGD'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}